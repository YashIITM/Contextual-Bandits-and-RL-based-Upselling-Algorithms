{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Use The Packages - NeuralUCB and NeuralTS and LinUCB - A Comparision ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMPORT STATEMENTS - Some Examples\n",
    "# from bluebandits.NeuralUCB.agent import *\n",
    "# from bluebandits.LinUCB.agent import *\n",
    "\n",
    "# from bluebandits.NeuralThompsonSampling import *\n",
    "# from bluesimulator.NeuralThompsonSampling.environment import *\n",
    "# from bluesimulator.NeuralUCB.environment import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import bluebandits as bb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Set the parameter of the network\n",
    "# the setting is based on the description of section 7.1 of the papaer\n",
    "L = 2\n",
    "m = 30 \n",
    "K = 4# Total number of actions, \n",
    "T = 2666 # Total number of periods\n",
    "d = 4 # the dimension of context\n",
    "# we fix gamma in each round, according to the description of section 3.1\n",
    "gamma_t = 0.01 #{0.01, 0.1, 1, 10}\n",
    "nu = 0.1 #{0.01, 0.1, 1}\n",
    "lambda_ = 1 #{0.1, 1, 10}\n",
    "delta = 0.01 #{0.01, 0.1, 1}\n",
    "S = 0.01 #{0.01, 0.1, 1, 10}\n",
    "eta = 1e-3 #{0.001, 0.01, 0.1}\n",
    "frequency = 50\n",
    "batchsize = 50\n",
    "verbose = False\n",
    "A = np.random.normal(loc=0, scale=1, size=(d, d))\n",
    "# Set the parameter of the network\n",
    "theta = np.empty(shape = (T,K,d))\n",
    "A1 = np.array([np.diag(np.ones(shape=d)) for _ in np.arange(K)])\n",
    "p = np.empty(shape = (T, K))\n",
    "X = np.array([[np.random.uniform(low=0, high=1, size=d) for _ in range(K)] for _ in np.arange(T)])\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action = []\n",
    "action_rank_UCB = []\n",
    "action_rank_TS = []\n",
    "action_rank_LinUCB = []\n",
    "action_list_UCB = []\n",
    "action_list_TS =[]\n",
    "action_list_LinUCB = []\n",
    "\n",
    "from bluebandits.NeuralUCB.agent import Agent\n",
    "neuralUCBagent = Agent(\n",
    "    K=K, T=T, d=d, L=L, m=m, gamma_t=gamma_t, nu=nu, lambda_=lambda_, delta=delta, S=S, eta=eta, frequency=frequency, batchsize=batchsize\n",
    ")\n",
    "from bluebandits.NeuralThompsonSampling.agent import Agent\n",
    "neuralTSagent = Agent(K=K, T=T, d=d, L=L, m=m, nu=nu, lambda_=lambda_, eta=eta, frequency=frequency, batchsize=batchsize)\n",
    "from bluebandits.LinUCB.agent import Agent\n",
    "linearagent = bb.LinUCB.agent.Agent(K = K,T=T,d=d,A=A1,theta=theta,X=X,p=p,alpha=alpha)\n",
    "for tt in tqdm(range(1, T + 1)):\n",
    "\n",
    "    # if tt %2000 == 1:\n",
    "    #     A = np.random.normal(loc=0, scale=1, size=(d, d))\n",
    "\n",
    "    \n",
    "    ''' God Mode'''\n",
    "    context_list = SampleContext(d, K)\n",
    "    realized_reward = GetRealReward(context_list, A)\n",
    "    best_action_for_customer = np.argmax(realized_reward)\n",
    "    best_action.append(best_action_for_customer)\n",
    "    action_order = np.flip(np.argsort(realized_reward))\n",
    "\n",
    "    ''' God Mode ends'''\n",
    "    \n",
    "    neuralUCB_ind = neuralUCBagent.Action(context_list)\n",
    "    action_list_UCB.append(neuralUCB_ind)\n",
    "    action_rank_UCB.append(list(action_order).index(neuralUCB_ind))\n",
    "\n",
    "    neuralTS_ind = neuralTSagent.Action(context_list)\n",
    "    action_list_TS.append(neuralTS_ind)\n",
    "    action_rank_TS.append(list(action_order).index(neuralTS_ind))\n",
    "\n",
    "    linearUCB_ind = linearagent.Action(context_list)\n",
    "    action_list_LinUCB.append(linearUCB_ind)\n",
    "    action_rank_LinUCB.append(list(action_order).index(linearUCB_ind))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ''' Online Learning '''\n",
    "    neuralUCB_reward = realized_reward[neuralUCB_ind]\n",
    "    neuralUCBagent.Update(neuralUCB_reward)\n",
    "\n",
    "    neuralTS_reward = realized_reward[neuralTS_ind]\n",
    "    neuralTSagent.Update(neuralTS_reward)\n",
    "\n",
    "    LinUCB_reward = realized_reward[linearUCB_ind]\n",
    "    linearagent.Update(LinUCB_reward)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

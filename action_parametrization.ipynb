{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide and Source code for the implementation of action parametrization ###\n",
    "#### Here, as an example action parametrization is done with NeuralUCB, for more kindly refer demo.ipynb ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import bluebandits as bb\n",
    "import bluesimulator as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Set the parameter of the network\n",
    "# the setting is based on the description of section 7.1 of the papaer\n",
    "L = 2\n",
    "m = 30 \n",
    "K = 4# Total number of actions,\n",
    "actions = np.random.normal(size=[K,2])\n",
    "T = 5000 # Total number of periods\n",
    "d = 4 # the dimension of context\n",
    "# we fix gamma in each round, according to the description of section 3.1\n",
    "gamma_t = 0.01 #{0.01, 0.1, 1, 10}\n",
    "nu = 0.1 #{0.01, 0.1, 1}\n",
    "lambda_ = 1 #{0.1, 1, 10}\n",
    "delta = 0.01 #{0.01, 0.1, 1}\n",
    "S = 0.01 #{0.01, 0.1, 1, 10}\n",
    "eta = 1e-3 #{0.001, 0.01, 0.1}\n",
    "frequency = 50\n",
    "batchsize = 50\n",
    "verbose = False\n",
    "A = np.random.normal(loc=0, scale=1, size=(d, d))\n",
    "# Set the parameter of the network\n",
    "theta = np.empty(shape = (T,K,d))\n",
    "A1 = np.array([np.diag(np.ones(shape=d)) for _ in np.arange(K)])\n",
    "p = np.empty(shape = (T, K))\n",
    "X = np.array([[np.random.uniform(low=0, high=1, size=d) for _ in range(K)] for _ in np.arange(T)])\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL UCB ALGORITHM\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d: int,\n",
    "        L: int = 2,\n",
    "        m: int = 20,\n",
    "        random_seed: int = 12345,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        \"\"\"The proposed neural network structure in Zhou 2020\n",
    "\n",
    "        Args:\n",
    "            d (int): Dimension of input layer.\n",
    "            L (int, optional): Number of Layers. Defaults to 2.\n",
    "            m (int, optional): Width of each layer. Defaults to 20.\n",
    "            random_seed (int, optional): rando_seed. Defaults to 12345.\n",
    "            device (torch.device, optional): The device of calculateing tensor. Defaults to torch.device(\"cpu\").\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.d = d\n",
    "        self.L = L\n",
    "        self.m = m\n",
    "        self.random_seed = random_seed\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "        self.device = device\n",
    "        print(f\"Using device {self.device}\")\n",
    "\n",
    "        self.W = torch.nn.ParameterDict()\n",
    "        w_for_1 = np.random.randn(d // 2, m // 2) * np.sqrt(4 / m)\n",
    "        w_for_1_to_Lminus1 = np.random.randn(m // 2, m // 2) * np.sqrt(4 / m)\n",
    "        w_for_L = np.random.randn(m // 2) * np.sqrt(2 / m)\n",
    "        for layer_index in range(1, L + 1):\n",
    "            if layer_index == 1:\n",
    "                W = np.zeros((d, m))\n",
    "                W[0 : d // 2, 0 : m // 2] = w_for_1\n",
    "                W[d // 2 :, m // 2 :] = w_for_1\n",
    "                self.W[\"W1\"] = torch.nn.Parameter(torch.from_numpy(W)).to(self.device)\n",
    "            elif layer_index == L:\n",
    "                W = np.zeros((m, 1))\n",
    "                W[0 : m // 2, 0] = w_for_L\n",
    "                W[m // 2 :, 0] = -w_for_L\n",
    "                self.W[f\"W{layer_index}\"] = torch.nn.Parameter(torch.from_numpy(W)).to(self.device)\n",
    "            else:\n",
    "                W = np.zeros((m, m))\n",
    "                W[0 : m // 2, 0 : m // 2] = w_for_1_to_Lminus1\n",
    "                W[m // 2 :, m // 2 :] = w_for_1_to_Lminus1\n",
    "                self.W[f\"W{layer_index}\"] = torch.nn.Parameter(torch.from_numpy(W)).to(self.device)\n",
    "        self.W0 = dict()\n",
    "        for key in self.W.keys():\n",
    "            self.W0[key] = deepcopy(self.W[key])\n",
    "            self.W0[key].requires_grad_(requires_grad=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The observed context of each arm\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The predicted mean reward of each arm\n",
    "        \"\"\"\n",
    "        assert x.shape[1] == self.d, \"Dimension doesn't match\"\n",
    "        x = x.to(self.device)\n",
    "        for layer_index in range(1, self.L + 1):\n",
    "            x = torch.matmul(x, self.W[f\"W{layer_index}\"])\n",
    "            if layer_index != self.L:\n",
    "                x = self.activation(x)\n",
    "        x = x * np.sqrt(self.m)\n",
    "        return x\n",
    "\n",
    "    def GetGrad(self, x: torch.tensor) -> np.ndarray:\n",
    "        \"\"\"Given the vector of context, return the flattern gradient of parameter\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): x.shape = (d,)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The gradient of parameter at given point\n",
    "        \"\"\"\n",
    "        x = x[None, :]  # expand the dimension of x\n",
    "        output = self.forward(x)[0, 0]\n",
    "        output.backward()\n",
    "\n",
    "        grad = np.array([])\n",
    "        for para in self.parameters():\n",
    "            grad = np.concatenate([grad, para.grad.cpu().detach().numpy().flatten()], axis=0)\n",
    "        return grad\n",
    "\n",
    "\n",
    "class BestAgent:\n",
    "    def __init__(self, K, T, d, A):\n",
    "        # K is Total number of actions,\n",
    "        # T is Total number of periods\n",
    "        # d is the dimension of context\n",
    "        # A is the context\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.t = 0  # marks the index of period\n",
    "        self.A = A\n",
    "        self.history_reward = np.zeros(T)\n",
    "        self.history_action = np.zeros(T)\n",
    "        self.history_context = np.zeros((d, T))\n",
    "\n",
    "    def Action(self, context_list):\n",
    "        # context_list is a d*K matrix, each column represent a context\n",
    "        # the return value is the action we choose, represent the index of action, is a scalar\n",
    "\n",
    "        expected_reward = np.zeros(self.K)\n",
    "        for kk in range(0, self.K):\n",
    "            context = context_list[kk, :]\n",
    "            expected_reward[kk] = context.transpose().dot(self.A.transpose().dot(self.A)).dot(context)\n",
    "        ind = np.argmax(expected_reward, axis=None)\n",
    "        self.history_context[:, self.t] = context_list[ind, :]\n",
    "        self.history_action[self.t] = ind\n",
    "        return ind\n",
    "\n",
    "    def Update(self, reward):\n",
    "        # reward is the realized reward after we adopt policy, a scalar\n",
    "        self.history_reward[self.t] = reward\n",
    "        self.t = self.t + 1\n",
    "\n",
    "    def GetHistoryReward(self):\n",
    "        return self.history_reward\n",
    "\n",
    "    def GetHistoryAction(self):\n",
    "        return self.history_action\n",
    "\n",
    "    def GetHistoryContext(self):\n",
    "        return self.history_context\n",
    "\n",
    "\n",
    "class UniformAgent:\n",
    "    def __init__(self, K, T, d):\n",
    "        # K is Total number of actions,\n",
    "        # T is Total number of periods\n",
    "        # d is the dimension of context\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.t = 0  # marks the index of period\n",
    "        self.history_reward = np.zeros(T)\n",
    "        self.history_action = np.zeros(T)\n",
    "        self.history_context = np.zeros((d, T))\n",
    "\n",
    "    def Action(self, context_list):\n",
    "        # context_list is a d*K matrix, each column represent a context\n",
    "        # the return value is the action we choose, represent the index of action, is a scalar\n",
    "\n",
    "        ind = np.random.randint(0, high=self.K)  # we just uniformly choose an action\n",
    "        self.history_context[:, self.t] = context_list[ind, :]\n",
    "        return ind\n",
    "\n",
    "    def Update(self, reward):\n",
    "        # reward is the realized reward after we adopt policy, a scalar\n",
    "        self.history_reward[self.t] = reward\n",
    "        self.t = self.t + 1\n",
    "\n",
    "    def GetHistoryReward(self):\n",
    "        return self.history_reward\n",
    "\n",
    "    def GetHistoryAction(self):\n",
    "        return self.history_action\n",
    "\n",
    "    def GetHistoryContext(self):\n",
    "        return self.history_context\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        K: int,\n",
    "        T: int,\n",
    "        d: int,\n",
    "        L: int = 2,\n",
    "        m: int = 20,\n",
    "        gamma_t: float = 0.01,\n",
    "        nu: float = 0.1,\n",
    "        lambda_: float = 0.01,\n",
    "        delta: float = 0.01,\n",
    "        S: float = 0.01,\n",
    "        eta: float = 0.001,\n",
    "        frequency: int = 50,\n",
    "        batchsize: int = 50,\n",
    "    ):\n",
    "        \"\"\"The proposed Neural UCB algorithm for solving contextual bandits\n",
    "\n",
    "        Args:\n",
    "            K (int): Number of arms\n",
    "            T (int): Number of rounds\n",
    "            d (int): Dimension of context\n",
    "            L (int, optional): Number of Layers. Defaults to 2.\n",
    "            m (int, optional): Width of each layer. Defaults to 20.\n",
    "            gamma_t (float, optional): Exploration parameter. Defaults to 0.01.\n",
    "            v (float, optional): Exploration parameter. Defaults to 0.1.\n",
    "            lambda_ (float, optional): Regularization parameter. Defaults to 0.01.\n",
    "            delta (float, optional): Confidence parameter. Defaults to 0.01.\n",
    "            S (float, optional): Norm parameter. Defaults to 0.01.\n",
    "            eta (float, optional): Step size. Defaults to 0.001.\n",
    "            frequency (int, optional): The interval between two training rounds. Defaults to 50.\n",
    "            batchsize (int, optional): The batchsize of applying SGD on the neural network. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "\n",
    "        self.L = L\n",
    "        self.m = m\n",
    "        self.gamma_t = gamma_t\n",
    "        self.nu = nu\n",
    "        self.lambda_ = lambda_\n",
    "        self.delta = delta\n",
    "        self.S = S\n",
    "        self.eta = eta\n",
    "        self.frequency = frequency  # we train the network after frequency, e.g. per 50 round\n",
    "        self.batchsize = batchsize\n",
    "        self.t = 0  # marks the index of period\n",
    "        self.history_reward = np.zeros(T)\n",
    "        self.history_action = np.zeros(T)\n",
    "        self.predicted_reward = np.zeros(T)\n",
    "        self.predicted_reward_upperbound = np.zeros(T)\n",
    "        self.history_context = np.zeros((T, d))\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mynn = NeuralNetwork(d=d, L=L, m=m, device=self.device)\n",
    "        self.optimizer = torch.optim.SGD(self.mynn.parameters(), lr=self.eta)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.p = m + m * d + m * m * (L - 2)\n",
    "        self.Z_t_minus1 = lambda_ * np.eye(self.p)\n",
    "\n",
    "    def Action(self, context_list: np.array) -> int:\n",
    "        \"\"\"Given the observed context of each arm, return the predicted arm\n",
    "\n",
    "        Args:\n",
    "            context_list (np.array): The observed context of each arm. context_list.shape = (K, d)\n",
    "\n",
    "        Returns:\n",
    "            int: the index of predicted arm, take value from 0, 1, ..., K-1\n",
    "        \"\"\"\n",
    "        predict_reward = self.mynn.forward(torch.from_numpy(context_list))[:, 0]\n",
    "        predict_reward = predict_reward.cpu().detach().numpy()\n",
    "\n",
    "        Z_t_minus1_inverse = np.linalg.inv(self.Z_t_minus1)\n",
    "\n",
    "        confidence = np.zeros(self.K)\n",
    "        for arm in range(1, self.K + 1):\n",
    "            grad_arm = self.mynn.GetGrad(torch.from_numpy(context_list[arm - 1, :]))\n",
    "            confidence[arm - 1] = np.sqrt(grad_arm.dot(Z_t_minus1_inverse).dot(grad_arm) / self.m)\n",
    "\n",
    "        # calculate the upper confidence bound\n",
    "        ucb = predict_reward + self.gamma_t * confidence\n",
    "        ind = np.argmax(ucb)\n",
    "\n",
    "        # save the history\n",
    "        self.history_action[self.t] = ind\n",
    "        self.history_context[self.t, :] = context_list[ind, :]\n",
    "        self.predicted_reward[self.t] = predict_reward[ind]\n",
    "        self.predicted_reward_upperbound = ucb[ind]\n",
    "        return ind\n",
    "\n",
    "    def Update(self, reward):\n",
    "        self.history_reward[self.t] = reward\n",
    "        ind = self.history_action[self.t]\n",
    "        context = self.history_context[self.t, :]\n",
    "\n",
    "        # compute Z_t_minus1\n",
    "        grad_parameter = self.mynn.GetGrad(torch.from_numpy(context))\n",
    "        grad_parameter = np.expand_dims(grad_parameter, axis=1)\n",
    "        self.Z_t_minus1 = self.Z_t_minus1 + grad_parameter.dot(grad_parameter.transpose()) / self.m\n",
    "\n",
    "        if (self.t + 1) % self.frequency == 0:  # train the network\n",
    "            # initialize the network again\n",
    "            for key in self.mynn.W.keys():\n",
    "                self.mynn.W[key].data = deepcopy(self.mynn.W0[key].data)\n",
    "\n",
    "            # for jj in range(self.t):  ## J=t at round t, but when we adopt such setting, the training process will be very slow\n",
    "            for jj in range(np.minimum(self.t, 100)):\n",
    "                loss_ = list()\n",
    "\n",
    "                # shuffle the history and conduct SGD\n",
    "                history_index = np.arange(self.t + 1)\n",
    "                np.random.shuffle(history_index)\n",
    "                temp_history_context = self.history_context[history_index, :]\n",
    "                temp_history_reward = self.history_reward[history_index]\n",
    "                for batch_index in range(0, self.t // self.batchsize + 1):\n",
    "                    # split the batch\n",
    "                    if batch_index < self.t // self.batchsize:\n",
    "                        X_temp = torch.from_numpy(temp_history_context[batch_index * self.batchsize : (batch_index + 1) * self.batchsize, :]).to(self.device)\n",
    "                        y_temp = torch.from_numpy(temp_history_reward[batch_index * self.batchsize : (batch_index + 1) * self.batchsize]).to(self.device)\n",
    "                    else:\n",
    "                        X_temp = torch.from_numpy(temp_history_context[batch_index * self.batchsize :, :]).to(self.device)\n",
    "                        y_temp = torch.from_numpy(temp_history_reward[batch_index * self.batchsize :]).to(self.device)\n",
    "\n",
    "                    # update the neural network\n",
    "                    self.optimizer.zero_grad()\n",
    "                    output = self.mynn.forward(X_temp)\n",
    "\n",
    "                    # calculate the loss function\n",
    "                    # in their orginal paper, $loss(\\theta)=\\sum_{i=1}^t(f(x_{i,a_i}, \\theta)-r_{i,a_i})^2+m\\lambda\\|\\theta-\\theta^{(0)}\\|_2^2/2$\n",
    "                    # but here we set $loss(\\theta)=\\sum_{i=1}^t(f(x_{i,a_i}, \\theta)-r_{i,a_i})^2/t+\\lambda\\|\\theta-\\theta^{(0)}\\|_2^2/2/p$\n",
    "                    # to balance the terms in the loss function\n",
    "                    loss = self.criterion(output[:, 0], y_temp)  ## predict error\n",
    "                    # loss = torch.sum((output[:, 0] - y_temp) ** 2)\n",
    "                    ## regularization\n",
    "                    for key in self.mynn.W.keys():\n",
    "                        # loss += self.lambda_ * self.m * torch.sum((self.mynn.W[key] - self.mynn.W0[key]) ** 2) / 2\n",
    "                        loss += self.lambda_ * torch.sum((self.mynn.W[key] - self.mynn.W0[key]) ** 2) / 2 / self.p\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # record the training process\n",
    "                    loss_.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                if (jj + 1) % 20 == 0:\n",
    "                    print(f\"{jj+1} training epoch, mean loss value is {np.mean(loss_)}\")\n",
    "\n",
    "        self.t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUERAL UCB ENVIRONMENT WITH ACTION PARAMETRIZATION\n",
    "import numpy as np\n",
    "from pandas import *\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class InconsistentNumberOfFeatures(Exception):\n",
    "    \"\"\"Exception raised for errors in the input features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, message=\"Inconsistent Dimensions of feature vectors:\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "def CustomContext(filename,list_of_features,d,K):\n",
    "    \"\"\"This function return context, as an K*d matrix, each row represents a context of action\n",
    "\n",
    "    Args:\n",
    "        filename: Name of custom data set\n",
    "        list_of_features: list of names of features that we wanna incorporate for the algorithm\n",
    "        d (int): Dimension of context\n",
    "        K (int): Number of arms\n",
    "\n",
    "    Returns:\n",
    "        context: an np.ndarray whose shape is (K, d), each row represents a context\n",
    "    \"\"\"\n",
    "    data = read_csv(filename)\n",
    "    if len(list_of_features)!=d:\n",
    "        raise InconsistentNumberOfFeatures()\n",
    "    features=[]\n",
    "    for feature in list_of_features:\n",
    "        temp_array = np.array(data[feature].to_list())\n",
    "        features.append(temp_array[0:K])\n",
    "    context = normalize(np.array(features),axis = 1,norm='l2')\n",
    "\n",
    "    return context\n",
    "\n",
    "def SampleContext(d, K):\n",
    "    \"\"\"This function return context, as an K*d matrix, each row represents a context of action\n",
    "\n",
    "    Args:\n",
    "        d (int): Dimension of context\n",
    "        K (int): Number of arms\n",
    "\n",
    "    Returns:\n",
    "        context: an np.ndarray whose shape is (K, d), each row represents a context\n",
    "    \"\"\"\n",
    "    context = np.random.normal(loc=0, scale=1, size=(K, d // 2))\n",
    "    length = np.sqrt(np.sum(context * context, axis=1, keepdims=True))\n",
    "    context = np.tile(context, (1, 2))\n",
    "    length = np.tile(length, (1, d))\n",
    "    context = context / length / np.sqrt(2)  # each column represent a context\n",
    "    return context\n",
    "\n",
    "\n",
    "def GetRealReward(context, A,actions):\n",
    "    \"\"\"Given the context, return the realized reward\n",
    "\n",
    "    Args:\n",
    "        context (np.ndarray): An np.ndarray whose shape is (K, d), each column represents a context of an arm\n",
    "        A (np.ndarray): The parameter of this reward function\n",
    "        actions(np.ndarray): A k*m dimensional action vectors\n",
    "    Returns:\n",
    "        reward: an np.ndarray whose shape is (K,), reward = context^T A^T A context + N(0, 0.05^2)\n",
    "    \"\"\"\n",
    "    # Concatenate action parameters to existing context list.\n",
    "    context = np.concatenate((context, actions), axis=1)\n",
    "    if len(context.shape) == 1:\n",
    "        return context.transpose().dot(A.transpose().dot(A)).dot(context) + np.random.normal(loc=0, scale=0.05)\n",
    "    else:\n",
    "        return np.diag(context.dot(A.transpose().dot(A)).dot(context.transpose())) + np.random.normal(loc=0, scale=0.05, size=context.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action = []\n",
    "action_rank_UCB = []\n",
    "action_rank_TS = []\n",
    "action_rank_LinUCB = []\n",
    "action_list_UCB = []\n",
    "action_list_TS =[]\n",
    "action_list_LinUCB = []\n",
    "\n",
    "neuralUCBagent = Agent(\n",
    "    K=K, T=T, d=d, L=L, m=m, gamma_t=gamma_t, nu=nu, lambda_=lambda_, delta=delta, S=S, eta=eta, frequency=frequency, batchsize=batchsize\n",
    ")\n",
    "for tt in tqdm(range(1, 11 + 1)):\n",
    "\n",
    "    # if tt %2000 == 1:\n",
    "    #     A = np.random.normal(loc=0, scale=1, size=(d, d))\n",
    "\n",
    "    \n",
    "    ''' God Mode'''\n",
    "    #context_list = CustomContext(filename = \"churn-bigml-80.csv\",list_of_features=['Account length','Total day charge','Total eve charge','Total night charge'],d=d, K=K)\n",
    "    context_list = bs.NeuralUCB.environment.SampleContext(d,K)\n",
    "    \n",
    "    realized_reward = GetRealReward(context_list, A,actions)\n",
    "    best_action_for_customer = np.argmax(realized_reward)\n",
    "    best_action.append(best_action_for_customer)\n",
    "    action_order = np.flip(np.argsort(realized_reward))\n",
    "\n",
    "    ''' God Mode ends'''\n",
    "    \n",
    "    neuralUCB_ind = neuralUCBagent.Action(context_list)\n",
    "    action_list_UCB.append(neuralUCB_ind)\n",
    "    action_rank_UCB.append(list(action_order).index(neuralUCB_ind))\n",
    "\n",
    "    ''' Online Learning '''\n",
    "    neuralUCB_reward = realized_reward[neuralUCB_ind]\n",
    "    neuralUCBagent.Update(neuralUCB_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "\n",
    "df = pd.DataFrame(action_rank_UCB)\n",
    "df['rank_Neural_UCB'] = action_rank_UCB\n",
    "df['rank_Neural_UCB'] = df['rank_Neural_UCB'].ewm(com=50).mean()\n",
    "df['time'] = np.arange(len(action_rank_UCB))\n",
    "plt.plot(df['time'],df['rank_Neural_UCB'],label='NeuralUCB')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"NeuralUCB with Action Parametrization\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

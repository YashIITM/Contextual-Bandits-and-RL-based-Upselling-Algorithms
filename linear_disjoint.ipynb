{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide and Source code for the implementation of linearUCB disjoint ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinUCB DISJOINT ENVIRONMENT CODE\n",
    "import numpy as np\n",
    "\n",
    "def make_design_matrix(n_trial, n_arms, n_feature):\n",
    "    \"\"\"\n",
    "    Returns the design matrix ofsize n_trial*n_arms*n_feature\n",
    "    \"\"\"\n",
    "    available_arms = np.arange(n_arms)\n",
    "    X = np.array([[np.random.uniform(low=0, high=1, size=n_feature) for _ in available_arms] for _ in np.arange(n_trial)])\n",
    "    return X\n",
    "\n",
    "def make_theta(n_arms, n_feature, best_arms, bias = 1):\n",
    "    \"\"\"\n",
    "    Returns the parameter matrix ofsize n_arms*n_feature\n",
    "    \"\"\"\n",
    "    true_theta = np.array([np.random.normal(size=n_feature, scale=1/4) for _ in np.arange(n_arms)])\n",
    "    true_theta[best_arms] = true_theta[best_arms] + bias\n",
    "    return true_theta\n",
    "\n",
    "def generate_reward(arm, x, theta, scale_noise = 1/10):\n",
    "    signal = theta[arm].dot(x)\n",
    "    noise = np.random.normal(scale=scale_noise)\n",
    "    return (signal + noise)\n",
    "\n",
    "def make_regret(payoff, oracle):\n",
    "    return np.cumsum(oracle - payoff)\n",
    "\n",
    "\n",
    "def GetRealReward(context: np.ndarray, theta: np.ndarray,scale_noise = 1/10) -> np.ndarray:\n",
    "    \"\"\"Given the context, return the realized reward\n",
    "\n",
    "    Args:\n",
    "        context (np.ndarray): An np.ndarray whose shape is (K, d), each column represents a context of an arm\n",
    "        theta is true theta(np.ndarray): The parameter of this reward function\n",
    "\n",
    "    Returns:\n",
    "        reward: an np.ndarray whose shape is (K,), reward = context^T A^T A context + N(0, 0.05^2)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for arm in range(theta.shape[0]):\n",
    "        signal = theta[arm].dot(context[arm])\n",
    "        noise = np.random.normal(scale=scale_noise)\n",
    "        rewards.append(noise+signal)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGENT CODE\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "class BestAgent:\n",
    "    def __init__(self, K, T, d, A):\n",
    "        # K is Total number of actions,\n",
    "        # T is Total number of periods\n",
    "        # d is the dimension of context\n",
    "        # A is the context\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.t = 0  # marks the index of period\n",
    "        self.A = A\n",
    "        self.history_reward = np.zeros(T)\n",
    "        self.history_action = np.zeros(T)\n",
    "        self.history_context = np.zeros((d, T))\n",
    "\n",
    "    def Action(self, context_list):\n",
    "        # context_list is a d*K matrix, each column represent a context\n",
    "        # the return value is the action we choose, represent the index of action, is a scalar\n",
    "\n",
    "        expected_reward = np.zeros(self.K)\n",
    "        for kk in range(0, self.K):\n",
    "            context = context_list[kk, :]\n",
    "            expected_reward[kk] = context.transpose().dot(self.A.transpose().dot(self.A)).dot(context)\n",
    "        ind = np.argmax(expected_reward, axis=None)\n",
    "        self.history_context[:, self.t] = context_list[ind, :]\n",
    "        self.history_action[self.t] = ind\n",
    "        return ind\n",
    "\n",
    "    def Update(self, reward):\n",
    "        # reward is the realized reward after we adopt policy, a scalar\n",
    "        self.history_reward[self.t] = reward\n",
    "        self.t = self.t + 1\n",
    "\n",
    "    def GetHistoryReward(self):\n",
    "        return self.history_reward\n",
    "\n",
    "    def GetHistoryAction(self):\n",
    "        return self.history_action\n",
    "\n",
    "    def GetHistoryContext(self):\n",
    "        return self.history_context\n",
    "\n",
    "\n",
    "class UniformAgent:\n",
    "    def __init__(self, K, T, d):\n",
    "        # K is Total number of actions,\n",
    "        # T is Total number of periods\n",
    "        # d is the dimension of context\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.t = 0  # marks the index of period\n",
    "        self.history_reward = np.zeros(T)\n",
    "        self.history_action = np.zeros(T)\n",
    "        self.history_context = np.zeros((d, T))\n",
    "\n",
    "    def Action(self, context_list: np.array) -> int:\n",
    "        # context_list is a d*K matrix, each column represent a context\n",
    "        # the return value is the action we choose, represent the index of action, is a scalar\n",
    "\n",
    "        ind = np.random.randint(0, high=self.K)  # we just uniformly choose an action\n",
    "        self.history_context[:, self.t] = context_list[ind, :]\n",
    "        return ind\n",
    "\n",
    "    def Update(self, reward):\n",
    "        # reward is the realized reward after we adopt policy, a scalar\n",
    "        self.history_reward[self.t] = reward\n",
    "        self.t = self.t + 1\n",
    "\n",
    "    def GetHistoryReward(self):\n",
    "        return self.history_reward\n",
    "\n",
    "    def GetHistoryAction(self):\n",
    "        return self.history_action\n",
    "\n",
    "    def GetHistoryContext(self):\n",
    "        return self.history_context\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        K: int,\n",
    "        T: int,\n",
    "        d: int,\n",
    "        A,\n",
    "        b,\n",
    "        theta,\n",
    "        X,\n",
    "        p,\n",
    "        alpha,\n",
    "        true_theta\n",
    "    ):\n",
    "        \"\"\"The proposed Neural UCB algorithm for solving contextual bandits\n",
    "\n",
    "        Args:\n",
    "            K (int): Number of arms\n",
    "            T (int): Number of rounds\n",
    "            d (int): Dimension of context\n",
    "            L (int, optional): Number of Layers. Defaults to 2.\n",
    "            m (int, optional): Width of each layer. Defaults to 20.\n",
    "            gamma_t (float, optional): Exploration parameter. Defaults to 0.01.\n",
    "            v (float, optional): Exploration parameter. Defaults to 0.1.\n",
    "            lambda_ (float, optional): Regularization parameter. Defaults to 0.01.\n",
    "            delta (float, optional): Confidence parameter. Defaults to 0.01.\n",
    "            S (float, optional): Norm parameter. Defaults to 0.01.\n",
    "            eta (float, optional): Step size. Defaults to 0.001.\n",
    "            frequency (int, optional): The interval between two training rounds. Defaults to 50.\n",
    "            batchsize (int, optional): The batchsize of applying SGD on the neural network. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.theta = theta\n",
    "        self.p = p\n",
    "        self.X = X\n",
    "        self.alpha = alpha\n",
    "        self.true_theta = true_theta\n",
    "        \n",
    "        self.t = 0  # marks the index of period\n",
    "        self.history_reward = np.zeros(T)\n",
    "        self.history_action = np.zeros(T)\n",
    "        self.predicted_reward = np.zeros(T)\n",
    "        self.predicted_reward_upperbound = np.zeros(T)\n",
    "        self.history_context = np.zeros((T, d))\n",
    "\n",
    "        \n",
    "\n",
    "    def Action(self, context_list: np.array,scale_noise = 1/10) -> int:\n",
    "        \"\"\"Given the observed context of each arm, return the predicted arm\n",
    "\n",
    "        Args:\n",
    "            context_list (np.array): The observed context of each arm. context_list.shape = (K, d)\n",
    "\n",
    "        Returns:\n",
    "            int: the index of predicted arm, take value from 0, 1, ..., K-1\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        \n",
    "        for arm in range(self.K):\n",
    "            inv_A = np.linalg.inv(self.A[arm])\n",
    "            self.theta[self.t,arm] = inv_A.dot(self.b[arm])\n",
    "            self.p[self.t,arm] = self.theta[self.t,arm].dot(self.X[self.t,arm]) + self.alpha*np.sqrt(self.X[self.t,arm].dot(inv_A).dot(self.X[self.t,arm]))\n",
    "\n",
    "\n",
    "        # calculate the upper confidence bound\n",
    "        chosen_arm = np.argmax(self.p[self.t])\n",
    "        ind = chosen_arm\n",
    "        x_chosen_arm = self.X[self.t, chosen_arm]\n",
    "        signal = self.true_theta[ind].dot(x_chosen_arm)\n",
    "        noise = np.random.normal(scale=scale_noise)\n",
    "        # save the history\n",
    "        self.history_action[self.t] = ind\n",
    "        self.history_context[self.t, :] = context_list[ind, :]\n",
    "        self.predicted_reward[self.t] = noise + signal\n",
    "        self.predicted_reward_upperbound = self.p[self.t][ind]\n",
    "        return ind\n",
    "\n",
    "    def Update(self, reward):\n",
    "        self.history_reward[self.t] = reward\n",
    "        ind = int(self.history_action[self.t])\n",
    "        \n",
    "        context = self.history_context[self.t, :]\n",
    "\n",
    "        temp_vec = self.A[ind]\n",
    "        self.A[ind] = temp_vec+ np.outer(self.X[self.t,ind],self.X[self.t,ind].T)\n",
    "        self.b+=reward*self.X[self.t,ind]\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import bluesimulator as bs\n",
    "\n",
    "\n",
    "# Implement the algorithm\n",
    "np.random.seed(12349)\n",
    "T = 5000\n",
    "K = 6\n",
    "d=10\n",
    "# Set the parameter of the network\n",
    "theta = np.empty(shape = (T,K,d))\n",
    "A = np.array([np.diag(np.ones(shape=d)) for _ in np.arange(K)])\n",
    "p = np.empty(shape = (T, K))\n",
    "X = make_design_matrix(T,K,d)\n",
    "alpha = 1\n",
    "b = np.array([np.zeros(shape=d) for _ in np.arange(K)])\n",
    "best_arms = [2]\n",
    "true_theta = make_theta(K,d,best_arms)\n",
    "\n",
    "\n",
    "linearagent = Agent(K = K,T=T,d=d,A=A,b=b,theta=theta,X=X,p=p,alpha=alpha,true_theta = true_theta)\n",
    "bestagent = BestAgent(K, T, d, A = A[0])\n",
    "uniformagent = UniformAgent(K, T, d)\n",
    "for tt in range(1, T + 1):\n",
    "    \n",
    "    # observe \\{x_{t,a}\\}_{a=1}^{k=1}\n",
    "    context_list = X[tt-1]\n",
    "    realized_reward = GetRealReward(context_list, true_theta)\n",
    "    \n",
    "    # neuralagent\n",
    "    linear_ind = linearagent.Action(context_list)# make a decision\n",
    "    linear_reward = realized_reward[linear_ind]# play neural_ind-th arm and observe reward\n",
    "    linearagent.Update(linear_reward)\n",
    "    \n",
    "    # bestagent\n",
    "    best_ind = bestagent.Action(context_list)# make a decision\n",
    "    best_reward = realized_reward[best_ind]# play best_ind-th arm and observe reward\n",
    "    bestagent.Update(best_reward)\n",
    "    \n",
    "    # uniformagent\n",
    "    uniform_ind = uniformagent.Action(context_list)# make a decision\n",
    "    uniform_reward = realized_reward[uniform_ind]# play uniform_ind-th arm and observe reward\n",
    "    uniformagent.Update(uniform_reward)\n",
    "    \n",
    "    print(\"round index {:d}; neural choose {:d}, reward is {:f}; best choose {:d}, reward is {:f}\".format(tt,\n",
    "                                                                                                          linear_ind,\n",
    "                                                                                                          linear_reward,\n",
    "                                                                                                          best_ind,\n",
    "                                                                                                          best_reward,))\n",
    "\n",
    "# plot the ratio of cumulated reward\n",
    "import matplotlib.pyplot as plt\n",
    "h_r_b = bestagent.GetHistoryReward()\n",
    "plt.plot(range(0, T), np.cumsum(h_r_b))\n",
    "\n",
    "h_r_u = uniformagent.GetHistoryReward()\n",
    "plt.plot(range(0, T), np.cumsum(h_r_u))\n",
    "\n",
    "h_r_n = deepcopy(linearagent.history_reward)\n",
    "plt.plot(range(0, T), np.cumsum(h_r_n))\n",
    "\n",
    "plt.legend([\"Best\", \"Uniform\", \"LinearUCB_Disjoint\"])\n",
    "plt.xlabel(\"Round Index\")\n",
    "plt.ylabel(\"Total Reward\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
